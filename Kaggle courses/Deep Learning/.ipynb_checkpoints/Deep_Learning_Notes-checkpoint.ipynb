{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fd8ccbc",
   "metadata": {},
   "source": [
    "# Lesson 1 A Single Neuron "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed425ecf",
   "metadata": {},
   "source": [
    "To create a **neural network** as a stack of layers use keraas.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9589a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create a network with 1 linear unit\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(units=1, input_shape=[3])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b647f1",
   "metadata": {},
   "source": [
    "**Why is input_shape a Python list?**\n",
    "\n",
    "The data we'll use in this course will be tabular data, like in a Pandas dataframe. We'll have one input for each feature in the dataset. The features are arranged by column, so we'll always have *input_shape=[num_columns].* The reason Keras uses a list here is to permit use of more complex datasets. Image data, for instance, might need three dimensions: [*height, width, channels].*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ee28a",
   "metadata": {},
   "source": [
    "### Implemented example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083beda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "red_wine = pd.read_csv('../input/dl-course-data/red-wine.csv')\n",
    "red_wine.head()\n",
    "red_wine.shape # (rows, columns) (1599, 12)\n",
    "input_shape =[11] # because the last column is a dependent variable\n",
    "\n",
    "\n",
    "model = keras.Sequential([layers.Dense(units=1, input_shape=[11])])\n",
    "\n",
    "x = tf.linspace(-1.0, 1.0, 100)\n",
    "y = model.predict(x)\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, y, 'k')\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.xlabel(\"Input: x\")\n",
    "plt.ylabel(\"Target y\")\n",
    "w, b = model.weights # you could also use model.get_weights() here\n",
    "plt.title(\"Weight: {:0.2f}\\nBias: {:0.2f}\".format(w[0][0], b[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ce4d7b",
   "metadata": {},
   "source": [
    "# Lesson 2 Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865661ac",
   "metadata": {},
   "source": [
    "Building Sequential Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c37201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # the hidden ReLU layers\n",
    "    layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
    "    layers.Dense(units=3, activation='relu'),\n",
    "    # the linear output layer \n",
    "    layers.Dense(units=1),\n",
    "])\n",
    "\n",
    "\n",
    "model =keras.Sequential([\n",
    "    layers.Dense(units=512, activation='relu', input_shape=[8]),\n",
    "    layers.Dense(units=512, activation='relu'),\n",
    "    layers.Dense(units=512, activation='relu'),\n",
    "    layers.Dense(units=1)\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be0a512",
   "metadata": {},
   "source": [
    "### Activation function\n",
    "Different way of implementing it on it's own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef50a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(32,  input_shape=[8]),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(32),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705bd58e",
   "metadata": {},
   "source": [
    "### Types of activation functions\n",
    "There is a whole family of variants of the 'relu' activation -- 'elu', 'selu', and 'swish'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61360976",
   "metadata": {},
   "source": [
    "# Lesson 3 Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b76f4",
   "metadata": {},
   "source": [
    "The loss function measures the disparity between the the target's true value and the value the model predicts.\n",
    "\n",
    "In addition to the training data, we need two more things:\n",
    "- A \"loss function\" that measures how good the network's predictions are.\n",
    "- An \"optimizer\" that can tell the network how to change its weights.\n",
    "\n",
    "Common loss functions are:\n",
    "- MAE - Mean Absolute Error\n",
    "- MSE - Mean Squared Error\n",
    "\n",
    "The optimizer is an algorithm that adjusts the weights to minimize the loss.Virtually all of the optimization algorithms used in deep learning belong to a family called stochastic gradient descent. They are iterative algorithms that train a network in steps. One step of training goes like this:\n",
    "- Sample some training data and run it through the network to make predictions.\n",
    "- Measure the loss between the predictions and the true values.\n",
    "- Finally, adjust the weights in a direction that makes the loss smaller.\n",
    "\n",
    "**A minibatch (or just batch)** is Each iteration's sample of training data.\n",
    "**Epoch** is a complete round of the training data.\n",
    "\n",
    "Every time SGD sees a new minibatch, it will **shift the weights** toward their correct values on that batch.\n",
    "\n",
    "The size of these shifts is determined by the **learning rate**. A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc1def51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=[11]),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d94ae",
   "metadata": {},
   "source": [
    "### Implemented example\n",
    "\n",
    "Including **preprocessing** with Standarization and One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5302f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "fuel = pd.read_csv('../input/dl-course-data/fuel.csv')\n",
    "\n",
    "X = fuel.copy()\n",
    "# Remove target\n",
    "y = X.pop('FE')\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(),\n",
    "     make_column_selector(dtype_include=np.number)),\n",
    "    (OneHotEncoder(sparse=False),\n",
    "     make_column_selector(dtype_include=object)),\n",
    ")\n",
    "\n",
    "X = preprocessor.fit_transform(X)\n",
    "y = np.log(y) # log transform target instead of standardizing\n",
    "\n",
    "input_shape = [X.shape[1]]\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "    layers.Dense(128, activation='relu'),    \n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "history = model.fit(X,y, batch_size=128,epochs=200)\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5. You can change this to get a different view.\n",
    "history_df.loc[5:, ['loss']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290872b",
   "metadata": {},
   "source": [
    "# Lesson 4 Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff3f24c",
   "metadata": {},
   "source": [
    "**Underfitting** the training set is when the loss is not as low as it could be because the model hasn't learned enough signal.\n",
    "\n",
    "**Overfitting** the training set is when the loss is not as low as it could be because the model learned too much noise. \n",
    "\n",
    "### Model Capacity\n",
    "A model's capacity refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity.\n",
    "\n",
    "You can increase the capacity of a network either by making it wider (more units to existing layers) or by making it deeper (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.\n",
    "\n",
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55611fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=20, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85453056",
   "metadata": {},
   "source": [
    "### Implemented example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d214b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=20, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=[11]),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc0c0b4",
   "metadata": {},
   "source": [
    "# Lesson 5 Dropout and Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dafa5e",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "To break up conspiracies in data, we randomly drop out some fraction of a layer's input units every step of training, making it much harder for the network to learn those spurious patterns in the training data. Instead, it has to search for broad, general patterns, whose weight patterns tend to be more robust.\n",
    "\n",
    "In Keras, the dropout rate argument *rate* defines what percentage of the input units to shut off. Put the Dropout layer just **before the layer** you want the dropout applied to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ee647",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.Sequential([\n",
    "    # ...\n",
    "    layers.Dropout(rate=0.3), # apply 30% dropout to the next layer\n",
    "    layers.Dense(16),\n",
    "    # ...\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59256b01",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "With neural networks, it's generally a good idea to put all of your data on a common scale, perhaps with something like scikit-learn's StandardScaler or MinMaxScaler. The reason is that SGD will shift the network weights in proportion to how large an activation the data produces. Features that tend to produce activations of very different sizes can make for unstable training behavior.\n",
    "\n",
    "To normalize the data inside the network can be done by **Batch Normalization Layer**. A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs.\n",
    "\n",
    "This layer can be putted at any point in the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf66459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after the layer:\n",
    "layers.Dense(16, activation='relu'),\n",
    "layers.BatchNormalization(),\n",
    "\n",
    "# between the layer and activation function:\n",
    "layers.Dense(16),\n",
    "layers.BatchNormalization(),\n",
    "layers.Activation('relu'),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b634b8d",
   "metadata": {},
   "source": [
    "And if you add it as the first layer of your network it can act as a kind of adaptive preprocessor, standing in for something like Sci-Kit Learn's *StandardScaler*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613f91d",
   "metadata": {},
   "source": [
    "### Implemented example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(1024, activation='relu', input_shape=[11]),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae',\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=256,\n",
    "    epochs=100,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "\n",
    "# Show the learning curves\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cb4b1c",
   "metadata": {},
   "source": [
    "# Lesson 6 Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f469a143",
   "metadata": {},
   "source": [
    "### Accuracy and Cross-Entropy\n",
    "Accuracy is one of the many metrics in use for measuring success on a classification problem. Accuracy is the ratio of correct predictions to total predictions: accuracy = number_correct / total. A model that always predicted correctly would have an accuracy score of 1.0. All else being equal, accuracy is a reasonable metric to use whenever the classes in the dataset occur with about the same frequency.\n",
    "\n",
    "The problem with accuracy (and most other classification metrics) is that it can't be used as a loss function. SGD needs a loss function that changes smoothly, but accuracy, being a ratio of counts, changes in \"jumps\". So, we have to choose a substitute to act as the loss function. This substitute is the cross-entropy function.\n",
    "\n",
    "Now, recall that the loss function defines the objective of the network during training. With regression, our goal was to minimize the distance between the expected outcome and the predicted outcome. We chose MAE to measure this distance.\n",
    "\n",
    "For classification, what we want instead is a distance between probabilities, and this is what cross-entropy provides. **Cross-entropy is a sort of measure for the distance from one probability distribution to another.**\n",
    "\n",
    "### Sigmoid \n",
    "The cross-entropy and accuracy functions both require probabilities as inputs, meaning, numbers from 0 to 1. To covert the real-valued outputs produced by a dense layer into probabilities, we attach a new kind of activation function, the sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a761373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(4, activation='relu', input_shape=[33]),\n",
    "    layers.Dense(4, activation='relu'),    \n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy'],\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=1000,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0, # hide the output because we have so many epochs\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
